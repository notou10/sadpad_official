 <!DOCTYPE html>
<html lang="en">
<head>
  <title>SaD_PaD</title>
  <meta name="description" content="Project page for SaD_PaD.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

<!--Facebook preview-->
  <meta property="og:image" content="resrc/celeba_church2.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <!-- <meta property="og:url" content="https://imagine.enpc.fr/~monniert/DTIClustering/"/> -->
  <meta property="og:title" content="SaD_PaD"/>
  <meta property="og:description" content="Project page for SaD_PaD"/>

  <!--Twitter preview-->
  <meta name="twitter:card" content="SaD_PaD" />
  <meta name="twitter:title" content="SaD_PaD" />
  <meta name="twitter:description" content="Project page for SaD_PaD."/>
  <meta name="twitter:image" content="resrc/celeba_church2.png"> 
  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

  <script> MathJax = { tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]} }; </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
 


<body>



<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>Attribute-Based Interpretable Metrics for Generative Models</h1>
    <h4>arXiv 2023</h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-12 col-md-6">
      <h4>
        <a href="https://vilab.yonsei.ac.kr/member/students"><nobr>Dongkyun Kim</nobr></a> &emsp;
        <a href="https://drive.google.com/file/d/1d1TOCA20KmYnY8RvBvhFwku7QaaWIMZL/view?usp=share_link"><nobr>Mingi Kwon</nobr></a> &emsp;
        <a href="https://vilab.yonsei.ac.kr/member/professor"><nobr>Youngjung Uh</nobr></a>
      </h4>
      Yonsei University</nobr>, Korea</nobr>
    </div>
    <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
      <a href="https://arxiv.org/abs/2310.17261" style="color:inherit" target="_blank">
        <i class="fa fa-file-pdf-o fa-4x"></i></a> 
    </div>
    <div class="hidden-xs hidden-sm col-md-2" style="text-align:left; margin-left:0px;">
      <a style="color:inherit" href="https://github.com/notou10/SaDPaD" target="_blank">
        <i class="fa fa-github fa-4x"></i></a>
    </div>
  </div>
</div>
 
<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond "diversity". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the <em>distribution of attribute strengths</em> as follows.</p>
        <p>Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding <em>joint</em> PDFs of a pair of attributes. They provide which attributes the models struggle. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with <em>heterogeneous initial points</em>.</p>

<!--         <ul>
            <li>ProjectedGAN generates implausible attribute relationships such as <code>baby</code> with <code>beard</code> even though it has competitive scores of existing metrics.</li>
            <li>Diffusion models struggle to capture diverse colors in the datasets.</li>
            <li>The larger sampling timesteps of latent diffusion model generate the more minor objects including <code>earrings</code> and <code>necklace</code>.</li>
            <li>Stable Diffusion v1.5 better captures the attributes than v2.1. Our metrics lay a foundation for explainable evaluations of generative models.</li>
        </ul> -->
</div>
 
<br></br>

 
<div class="container">
 <h3>Our metric has interpretablity!</h3>
 <hr/>
 <h4>Our metric can tell you Model 2 is worse than Model 1 <u>because it over-generates makeup, long hair, etc. </u></h4>

<div style="text-align : center;">
    <img src="overall-1.png" style="width: 80%"/>
  </div>
<p><strong>Figure 1</strong> illustrates the evaluation metrics for two models with distinct properties. While Model 1’s generated images align closely with the training dataset, Model 2 exhibits a lack of diversity. Notably, in Figure 1a (gray box), Model 1 consistently outperforms Model 2 across all metrics. Yet, these metrics fall short in explicability; for example, they don’t highlight the overrepresentation of <code>long hair</code> and <code>makeup</code> in Model 2.</p>
<p>Addressing this gap, our paper proposes a methodology to quantify discrepancies between generated and training images, focusing on specific attributes. Figure 1b shows the concept of our alternative approach that measures the distribution of attribute strengths compared to the training set: while Model 1 offers a balanced attribute distribution akin to the training dataset, Model 2 overemphasizes <code>long hair</code> and underrepresents <code>beard</code>.</p>
 
</div>

<br></br>
 
<div class="container">
 <h3>How does it works?</h3>
 <hr/>
<div style="text-align : center;">
    <img src="12.PNG" style="width: 80%"/>
  </div>
  <p><strong>Figure 2</strong> illustrates an overview of our metrics. We measure attribute strengths in all images in both two sets, and then calculate the divergence between two sets for each attributes.

  <br></br>
 
 <h4>We compare distribution of attribute strengths between two sets. </h4>
 <p><strong>Single-attribute Divergence (SaD) </strong> measures how much a generative model deviates from the distribution of each attribute in the training data. Such as if we have a dataset with dogs and cats, and a generative model only makes dog images, it is not an ideal model because it does not produce cats at all.

   <p><strong>Paired-attribute Divergence (PaD) </strong> measures how much a generative model breaks the relationship between attributes in the training data, such as "babies do not have beards."

    
 <br></br>
 <h4>Measuring attributes strengths. </h4>
  <p>To construct metrics that quantify the differences between two sets of images in an intelligible way, we introduce Heterogeneous CLIPScore (HCS), an advanced variant of CLIPScore. Unlike CLIPScore, HCS captures the similarity between two modalities—image and text—by setting distinct starting points for text and image vectors.</p>
 
 <div style="text-align : center;">
    <img src="2_9-1.png" style="width: 80%"/>
  <p><strong>Figure 3</strong> illustrates CLIPScore and Heterogeneous CLIPScore,  </strong> (a) CLIPScore (CS) evaluates the similarity between 
                \(V^{CS}_{img}\) and \(V^{CS}_{Text}\) from the coordinate origin, where the angle between the two vectors 
                is bounded, resulting in a limited similarity value. (b) HCS gauges the similarity between 
                \(V^{HCS}_{img}\) and \(V^{HCS}_{Text}\) using the defined means of images \(C_\mathcal{X}\) and texts 
                \(C_\mathcal{A}\) as the origin, the range of similarity is unrestricted. (c) shows flexible values of 
                HCS compared to CS.</p>
  
  </div>
 </div>


 <br></br>
 
 <div class="container">
 <h3>What did we uncover?</h3>
  <hr/>
 <p>
   <strong>Comparing the performance of generative models.</strong> We computed each generative model's performance on our metric with their official pretrained checkpoints on FFHQ. We used 50,000 images for both GT and the generated set. We used USER attributes for this experiment.
 </p>
         <div style="text-align : center;">
     <img src="7.PNG" style="width: 80%"/>
          </div>
    
        <p>
            Leveraging the superior sensitivity and discernment of our proposed metrics, 
            we evaluate the performance of GANs and Diffusion Models (DMs) in 
            Table above, the tendency of SaD and PaD 
            align with other existing metrics. However four notable points emerge;
        </p>
        <ol>
            <li>
                <strong>ProjectedGAN</strong> <a href="https://arxiv.org/abs/2107.00641" target="_blank">[Sauer et al., 2021]</a> lags in performance, having been criticized 
                <a href="https://arxiv.org/abs/2202.11582" target="_blank">[Kynkäänniemi et al., 2022]</a> for not focusing on fidelity and only aiming to achieve a good FID score 
                by training to match the training set's embedding statistics. Even though it exhibits satisfactory results in traditional metrics, 
                it notably underperforms in SaD and especially PaD when evaluated by our measures. This indicates that mimicking the training set's embedding statistics directly 
                does not necessarily capture the correlation of attributes on the training set.
            </li>
         <br></br>
     

            <li>
                Diffusion models typically yield better quality with increased sampling timesteps. However, SaD and PaD scores for LDM(200 steps) surpass those of LDM(50 steps). 
              <div style="text-align : center;">
     <img src="5_4-1.png" style="width: 80%"/>
          <p><strong>LDM with 50 steps v.s. LDM with 200 timesteps.</strong> With increased sampling timesteps, 
                    (a) SaD of LDM gets worse, 
                    (b) since making too many fine objects such as <code>earrings</code> or <code>necklace</code>. </p>
   </div>
                Higher sampling timesteps in the LDM model produce more high-frequency elements, such as <code>necklaces</code> and <code>earrings</code>. 
                This might elucidate the prevalence of attributes like <code>young, makeup, woman, wavy hair</code>. We posit that a dense sampling trajectory engenders more high-frequency objects.
            </li>
         <br></br>
            <li>
                Through a detailed analysis of SaD and PaD scores, we noticed divergent trends between the strengths and weaknesses of GANs and Diffusion models. To probe deeper, 
                we exploited the versatility of constructing attributes, examining score variations based on attribute characteristics. Specifically, we built attributes 
                focused on color (e.g., <code>yellow fur</code>, <code>black fur</code>) and on shape (e.g., <code>pointy ears</code>, <code>long tail</code>) within the LSUN Cat dataset.
            </li>
         <br></br>

          <li>
            <strong>Comparative analysis of StableDiffusion versions</strong> shows SDv1.5 has better attribute distribution than SDv2.1, 
            being almost twice as superior. (We generate 30k images using the caption of COCO. We use \( N_{\mathcal{A}}=30 \).) 
           
      <div style="text-align : center;">
        <img src="10.PNG" style="width: 60%"/>
       </div>
               Intriguingly, all SaD worst-rank attributes have negative mean differences (redundant), 
            indicating that SDs tend to omit certain objects such as <code>group</code><sup><a href="#fn1" id="ref1">1</a></sup> or 
            <code>plate</code><sup><a href="#fn2" id="ref2">2</a></sup>. Particularly, SDv2.1 faces challenges in generating multiple people. 
            This concurs with common assertions about SDv2.1, even though it reports a better FID score. In certain instances, users have noted 
            SDv2.1's inferior performance compared to SDv1.5<sup><a href="#fn3" id="ref3">3</a></sup>. More details are provided in 
            Appendix <a href="#subsec:coco_detail">A.1</a>.

            <p>
            <sup id="fn1">1</sup> e.g., A group of people is standing around a large clock. <a href="#ref1" title="Jump back to footnote 1 in the text.">↩</a><br>
            <sup id="fn2">2</sup> e.g., A table is set with two plates of food and a candle. <a href="#ref2" title="Jump back to footnote 2 in the text.">↩</a><br>
            <sup id="fn3">3</sup> For example, some users have reported SDv2.1's lower performance compared to SDv1.5 in a blog post 
            available at <a href="https://www.assemblyai.com/blog/stable-diffusion-1-vs-2-what-you-need-to-know/" target="_blank">AssemblyAI Blog</a>. 
            <a href="#ref3" title="Jump back to footnote 3 in the text.">↩</a>
        </p>
           
            </li>
         
        </ol>
  <br></br>
        <p>
            For more detailed insights and contextual understanding of the findings, 
            we encourage reading the full 
            <a href="https://arxiv.org/abs/2310.17261" target="_blank">research paper</a>.
        </p>

 
  
 </div>



 

</body>
</html>
