 <!DOCTYPE html>
<html lang="en">
<head>
  <title>SaD_PaD</title>
  <meta name="description" content="Project page for SaD_PaD.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

<!--   <!--Facebook preview-->
  <meta property="og:image" content="resrc/celeba_church2.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <!-- <meta property="og:url" content="https://imagine.enpc.fr/~monniert/DTIClustering/"/> -->
  <meta property="og:title" content="Asyrp"/>
  <meta property="og:description" content="Project page for Asyrp."/>

  <!--Twitter preview-->
  <meta name="twitter:card" content="Asyrp" />
  <meta name="twitter:title" content="Asyrp" />
  <meta name="twitter:description" content="Project page for Asyrp."/>
  <meta name="twitter:image" content="resrc/celeba_church2.png"> -->

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

  <script> MathJax = { tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]} }; </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
 


<body>



<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>Attribute Based Interpretable Metrics for Generative Models</h1>
    <h4>arXiv 2024</h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-12 col-md-6">
      <h4>
        <a href="https://vilab.yonsei.ac.kr/member/students"><nobr>Dongkyun Kim</nobr></a> &emsp;
        <a href="https://drive.google.com/file/d/1d1TOCA20KmYnY8RvBvhFwku7QaaWIMZL/view?usp=share_link"><nobr>Mingi Kwon</nobr></a> &emsp;
        <a href="https://vilab.yonsei.ac.kr/member/professor"><nobr>Youngjung Uh</nobr></a>
      </h4>
      Yonsei University</nobr>, Korea</nobr>
    </div>
    <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
      <a href="https://arxiv.org/pdf/2210.10960.pdf" style="color:inherit" target="_blank">
        <i class="fa fa-file-pdf-o fa-4x"></i></a> 
    </div>
    <div class="hidden-xs hidden-sm col-md-2" style="text-align:left; margin-left:0px;">
      <a style="color:inherit" href="https://github.com/notou10/SaDPaD" target="_blank">
        <i class="fa fa-github fa-4x"></i></a>
    </div>
  </div>
</div>
 
<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond "diversity". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the <em>distribution of attribute strengths</em> as follows.</p>
        <p>Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding <em>joint</em> PDFs of a pair of attributes. They provide which attributes the models struggle. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with <em>heterogeneous initial points</em>.</p>
        <p>With SaD and PaD, we reveal the following about existing generative models.</p>
<!--         <ul>
            <li>ProjectedGAN generates implausible attribute relationships such as <code>baby</code> with <code>beard</code> even though it has competitive scores of existing metrics.</li>
            <li>Diffusion models struggle to capture diverse colors in the datasets.</li>
            <li>The larger sampling timesteps of latent diffusion model generate the more minor objects including <code>earrings</code> and <code>necklace</code>.</li>
            <li>Stable Diffusion v1.5 better captures the attributes than v2.1. Our metrics lay a foundation for explainable evaluations of generative models.</li>
        </ul> -->
</div>

 
<div class="container">
 <h3>Our metric has interpretablity</h3>
 <hr/>
 <h4>Our metric can tell you Model 2 is worse than Model <u>because it over-generates make-up, long hair, etc. </u></h4>

<div style="text-align : center;">
    <img src="overall-1.png" style="width: 80%"/>
  </div>
<p><strong>Figure 1</strong> illustrates the evaluation metrics for two models with distinct properties. While Model 1’s generated images align closely with the training dataset, Model 2 exhibits a lack of diversity. Notably, in Figure 1a (gray box), Model 1 consistently outperforms Model 2 across all metrics. Yet, these metrics fall short in explicability; for example, they don’t highlight the overrepresentation of <code>long hair</code> and <code>makeup</code> in Model 2.</p>
<p>Addressing this gap, our paper proposes a methodology to quantify discrepancies between generated and training images, focusing on specific attributes. Figure 1b shows the concept of our alternative approach that measures the distribution of attribute strengths compared to the training set: while Model 1 offers a balanced attribute distribution akin to the training dataset, Model 2 overemphasizes <code>long hair</code> and underrepresents <code>beard</code>.</p>
 
</div>


<div class="container">
 <h3>How it works?</h3>
 <hr/>
  <p>To construct metrics that quantify the differences between two sets of images in an intelligible way, we introduce Heterogeneous CLIPScore (HCS), an advanced variant of CLIPScore. Unlike CLIPScore, HCS captures the similarity between two modalities—image and text—by setting distinct starting points for text and image vectors.</p>
 
 <div style="text-align : center;">
    <img src="2_9-1.png" style="width: 80%"/>
  <p><strong>Figure 2</strong>Illustration of CLIPScore and HCS.</strong> (a) CLIPScore (CS) evaluates the similarity between 
                \(V^{CS}_{img}\) and \(V^{CS}_{Text}\) from the coordinate origin, where the angle between the two vectors 
                is bounded, resulting in a limited similarity value. (b) HCS gauges the similarity between 
                \(V^{HCS}_{img}\) and \(V^{HCS}_{Text}\) using the defined means of images \(C_\mathcal{X}\) and texts 
                \(C_\mathcal{A}\) as the origin, the range of similarity is unrestricted. (c) shows flexible values of 
                HCS compared to CS.</p>
  
  </div>
 </div>


 <div class="container">
 <h3>So, what did we uncover?</h3>
  <hr/>
 <p>
   <strong>Comparing the performance of generative models.</strong> We computed each generative model's performance on our metric with their official pretrained checkpoints on FFHQ. We used 50,000 images for both GT and the generated set. We used USER attributes for this experiment.
 </p>

   <table border="1" style="width:100%; text-align:center;">
                <tr>
                    <th></th>
                    <th>StyleGAN1</th>
                    <th>StyleGAN2</th>
                    <th>StyleGAN3</th>
                    <th>iDDPM</th>
                    <th>LDM (50)</th>
                    <th>LDM (200)</th>
                    <th>StyleSwin</th>
                    <th>ProjectedGAN</th>
                </tr>
                <tr>
                    <td><b>SaD (↓1e-7</sup>)</b></td>
                    <td>11.35</td>
                    <td><strong>7.52</strong></td>
                    <td>7.79</td>
                    <td>14.78</td>
                    <td>10.42</td>
                    <td>14.04</td>
                    <td>10.76</td>
                    <td>17.61</td>
                </tr>
                <tr>
                    <td><b>PaD (↓1e-7</sup>)</b></td>
                    <td>27.25</td>
                    <td><strong>19.22</strong></td>
                    <td>19.73</td>
                    <td>34.04</td>
                    <td>25.36</td>
                    <td>30.71</td>
                    <td>26.56</td>
                    <td>41.53</td>
                </tr>
                <tr>
                    <td>FID (↓)</td>
                    <td>4.74</td>
                    <td><strong>3.17</strong></td>
                    <td>3.20</td>
                    <td>7.31</td>
                    <td>12.18</td>
                    <td>11.86</td>
                    <td>4.45</td>
                    <td>5.45</td>
                </tr>
                <tr>
                    <td>FID<sub>CLIP</sub> (↓)</td>
                    <td>3.17</td>
                    <td><strong>1.47</strong></td>
                    <td>1.66</td>
                    <td>2.39</td>
                    <td>3.89</td>
                    <td>3.57</td>
                    <td>2.45</td>
                    <td>3.63</td>
                </tr>
                <tr>
                    <td>Precision (↑)</td>
                    <td>0.90</td>
                    <td>0.92</td>
                    <td>0.92</td>
                    <td>0.93</td>
                    <td><strong>0.94</strong></td>
                    <td>0.91</td>
                    <td>0.92</td>
                    <td>0.92</td>
                </tr>
                <tr>
                    <td>Recall (↑)</td>
                    <td>0.86</td>
                    <td>0.89</td>
                    <td>0.90</td>
                    <td>0.84</td>
                    <td>0.82</td>
                    <td>0.88</td>
                    <td>0.91</td>
                    <td><strong>0.92</strong></td>
                </tr>
                <tr>
                    <td>Density (↑)</td>
                    <td>1.05</td>
                    <td>1.03</td>
                    <td>1.03</td>
                    <td><strong>1.09</strong></td>
                    <td>1.09</td>
                    <td>1.07</td>
                    <td>1.01</td>
                    <td>1.05</td>
                </tr>
                <tr>
                    <td>Coverage (↑)</td>
                    <td>0.97</td>
                    <td>0.97</td>
                    <td>0.97</td>
                    <td>0.95</td>
                    <td>0.94</td>
                    <td>0.97</td>
                    <td>0.97</td>
                    <td>0.97</td>
                </tr>
            </table>
        </figure>
    
    
        <p>
            Leveraging the superior sensitivity and discernment of our proposed metrics, 
            we evaluate the performance of GANs and Diffusion Models (DMs) in 
            Table \( \ref{tab:gan_diff_blip} \). Generally, the tendency of SaD and PaD 
            align with other existing metrics. However three notable points emerge;
        </p>
        <ol>
            <li>
                <strong>ProjectedGAN</strong> <a href="https://arxiv.org/abs/2107.00641" target="_blank">[Sauer et al., 2021]</a> lags in performance, having been criticized 
                <a href="https://arxiv.org/abs/2202.11582" target="_blank">[Kynkäänniemi et al., 2022]</a> for not focusing on fidelity and only aiming to achieve a good FID score 
                by training to match the training set's embedding statistics. Even though it exhibits satisfactory results in traditional metrics, 
                it notably underperforms in SaD and especially PaD when evaluated by our measures. This indicates that mimicking the training set's embedding statistics directly 
                does not necessarily capture the correlation of attributes on the training set. Figure \( \ref{fig:1d_2d_difference} \)b provides instances where ProjectedGAN falls short.
            </li>

            <figure class="center-figure">
                <img src="imgs/ldm_sampling_step.png" alt="LDM comparison between 50 and 200 timesteps" class="center-image" />
                <figcaption>
                    <strong>LDM with 50 steps v.s. LDM with 200 timesteps.</strong> With increased sampling timesteps, 
                    (a) SaD of LDM gets worse, 
                    (b) since making too many fine objects such as <code>earrings</code> or <code>necklace</code>.
                </figcaption>
            </figure>
            

            <li>
                Diffusion models typically yield better quality with increased sampling timesteps. However, SaD and PaD scores for LDM(200 steps) surpass those of LDM(50 steps). 
                As illustrated in Figure \( \ref{fig:5} \), higher sampling timesteps in the LDM model produce more high-frequency elements, such as <code>necklaces</code> and <code>earrings</code>. 
                This might elucidate the prevalence of attributes like <code>young, makeup, woman, wavy hair</code>. We posit that a dense sampling trajectory engenders more high-frequency objects.
            </li>
            <li>
                Through a detailed analysis of SaD and PaD scores, we noticed divergent trends between the strengths and weaknesses of GANs and Diffusion models. To probe deeper, 
                we exploited the versatility of constructing attributes, examining score variations based on attribute characteristics. Specifically, we built attributes 
                focused on color (e.g., <code>yellow fur</code>, <code>black fur</code>) and on shape (e.g., <code>pointy ears</code>, <code>long tail</code>) within the LSUN Cat dataset.
            </li>
        </ol>
        <p>
            For more detailed insights and contextual understanding of the findings, 
            we encourage reading the full 
            <a href="your_paper_link_here" target="_blank">research paper</a>.
        </p>
  
  
 </div>
  
  

 

 
 

 

</body>
</html>
